{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark-sql-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('DataFrame_2') \\\n",
    "    .master('local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/apache/spark\n",
    "data_path = ''\n",
    "people = spark.read.json(data_path+'people.json')\n",
    "employees = spark.read.json(data_path+'employees.json')\n",
    "people_txt = spark.read.option(\"inferSchema\", \"true\").csv(data_path+'people.txt')\n",
    "people_txt = people_txt.withColumnRenamed('_c0', 'name').withColumnRenamed('_c1', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|29.0|\n",
      "|   Andy|30.0|\n",
      "| Justin|19.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_txt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join\n",
    "inner (domyslny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|salary|\n",
      "+-------+----+------+\n",
      "|Michael|null|  3000|\n",
      "|   Andy|  30|  4500|\n",
      "| Justin|  19|  3500|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.join(other=employees, on='name', how='inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga ogolna\n",
    "Join to stosunkowo popularna, ale kosztowna operacja.\n",
    "W sytuacji, kiedy jeden z laczonych DataFramow jest znacznie mniejszy (w szczegolnosci na tyle maly, ze w calosci miesci sie w pamieci), zaleca sie zastosowanie broadcast hash join.\n",
    "(Mala tabela zostanie zebrana do pamieci i wyslana do kazdego noda).\n",
    "W niektorych przypadkach optymalizator sam za nas zdecyduje o zastosowaniu broadcast hash join. Jednak SparkSQL wyjatkowo tutaj daje nam mozliwosc wymuszenia tej operacji wprost w kodzie. \n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Znajdź ile zarabia najmlodsza osoba spośród (people, people_txt)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|  3500|\n",
      "|  3000|\n",
      "|  4500|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_txt.join(other=employees, on='name', how='inner').select('salary').orderBy(people_txt.age.asc()).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_order=people.columns\n",
    "joined_df=people.union(people_txt.select(columns_order)).\\\n",
    "join(employees,'name').dropna().orderBy('age')\n",
    "joined_df.select('salary').first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Dla kazdego pracownika (employees), dla ktorego mamy informacje o wieku (people, people_txt) dodaj do pensji 0.1% za kazdy rok zycia. Zsumuj koszt takiego 'bonusu urodzinowego' dla pracodawcy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+------------------+-----------------+\n",
      "|   name| age|salary|       bonusSalary|            bonus|\n",
      "+-------+----+------+------------------+-----------------+\n",
      "|Michael|29.0|  3000|3086.9999999999995|86.99999999999955|\n",
      "|   Andy|30.0|  4500|            4635.0|            135.0|\n",
      "| Justin|19.0|  3500|3566.4999999999995|66.49999999999955|\n",
      "+-------+----+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_order=people.columns\n",
    "joined_df=people.union(people_txt.select(columns_order)).join(employees,'name').dropna()\n",
    "joined_df=joined_df.drop_duplicates()\n",
    "joined_df=joined_df.withColumn(colName = 'bonusSalary', col = joined_df.salary*(1+0.001*joined_df.age))\n",
    "joined_df=joined_df.withColumn(colName = 'bonus', col = joined_df.salary*(1+0.001*joined_df.age)-joined_df.salary)\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       sum(bonus)|\n",
      "+-----------------+\n",
      "|288.4999999999991|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.select(f.sum('bonus')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x2364f3b3b20>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.groupBy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Przez GroupedData mamy dostep do takich funkcji jak:<br>\n",
    " avg, max, min, sum, count, agg <br>\n",
    " (dla wygody, do funkcji 'agg' mamy tez dostep bezposrednio na DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      30|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy().max('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   name|count|\n",
      "+-------+-----+\n",
      "|Michael|    1|\n",
      "|   Andy|    1|\n",
      "| Justin|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy('name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+--------+\n",
      "|   name|min_age|max_age|n_people|\n",
      "+-------+-------+-------+--------+\n",
      "|Michael|   null|   null|       1|\n",
      "|   Andy|     30|     30|       1|\n",
      "| Justin|     19|     19|       1|\n",
      "+-------+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql import functions as f\n",
    "people.groupBy('name').agg(f.min('age').alias('min_age'), f.max('age').alias('max_age'), f.count('name').alias('n_people')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----------+\n",
      "|min(age)|max(age)|count(name)|\n",
      "+--------+--------+-----------+\n",
      "|      19|      30|          3|\n",
      "+--------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.agg(f.min('age'), f.max('age'), f.count('name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Ile jest unikatowych (wystepujacych tylko 1 raz) imion w polaczonych zbiorach people oraz people_txt? (allPeople)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   name|count|\n",
      "+-------+-----+\n",
      "|Michael|    2|\n",
      "|   Andy|    2|\n",
      "| Justin|    2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_order=people.columns\n",
    "countedNames = people.union(people_txt.select(columns_order)).dropna(subset='name').groupBy('name').count()\n",
    "countedNames.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countedNames.where(countedNames['count']==1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Ile lat maja osoby, ktorych imiona wystepuja tylko raz w polaczonych zbiorach people oraz people_txt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wygenerujmy nowy DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = 10\n",
    "names = ['Alice', 'Betty', 'Chris', 'Dan', 'Greg']\n",
    "unique_names_count = len(names)\n",
    "names = sorted(names*years)\n",
    "year = [y for y in range(2000, 2000+years)]*len(names)\n",
    "starting_salary = [round(random.gauss(4000, 1000),2) for i in range(unique_names_count)]\n",
    "salary = [0 for i in range(years*unique_names_count)]\n",
    "salary[::years] = starting_salary\n",
    "for n in range(unique_names_count):\n",
    "    for y in range(years-1):\n",
    "        index = (years*n+1)+y\n",
    "        #print(index, salary[index-1])\n",
    "        salary[index] = round(salary[index-1]*(1+random.gauss(0.1,0.09)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory = spark.createDataFrame([Row(name=n, year=y, salary=s) for n,y,s in zip(names, year, salary)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory = salaryHistory.filter((salaryHistory['name'] != 'Greg') | (salaryHistory['year'] != 2006))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory = salaryHistory.union(spark.createDataFrame([Row('Alice', 3000, 2000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', year=2000, salary=4711.77),\n",
       " Row(name='Alice', year=2001, salary=4133.36),\n",
       " Row(name='Alice', year=2002, salary=4815.05),\n",
       " Row(name='Alice', year=2003, salary=5488.72),\n",
       " Row(name='Alice', year=2004, salary=6044.3),\n",
       " Row(name='Alice', year=2005, salary=7361.99),\n",
       " Row(name='Alice', year=2006, salary=8419.89),\n",
       " Row(name='Alice', year=2007, salary=9677.49),\n",
       " Row(name='Alice', year=2008, salary=9427.13),\n",
       " Row(name='Alice', year=2009, salary=10582.08),\n",
       " Row(name='Betty', year=2000, salary=4093.11),\n",
       " Row(name='Betty', year=2001, salary=4578.24),\n",
       " Row(name='Betty', year=2002, salary=4761.09),\n",
       " Row(name='Betty', year=2003, salary=4969.06),\n",
       " Row(name='Betty', year=2004, salary=5631.39),\n",
       " Row(name='Betty', year=2005, salary=6335.63),\n",
       " Row(name='Betty', year=2006, salary=6568.63),\n",
       " Row(name='Betty', year=2007, salary=7427.03),\n",
       " Row(name='Betty', year=2008, salary=9309.8),\n",
       " Row(name='Betty', year=2009, salary=9157.87),\n",
       " Row(name='Chris', year=2000, salary=5667.09),\n",
       " Row(name='Chris', year=2001, salary=5760.78),\n",
       " Row(name='Chris', year=2002, salary=6190.29),\n",
       " Row(name='Chris', year=2003, salary=6520.11),\n",
       " Row(name='Chris', year=2004, salary=7394.32),\n",
       " Row(name='Chris', year=2005, salary=8164.76),\n",
       " Row(name='Chris', year=2006, salary=9649.23),\n",
       " Row(name='Chris', year=2007, salary=10848.18),\n",
       " Row(name='Chris', year=2008, salary=9861.87),\n",
       " Row(name='Chris', year=2009, salary=10854.67),\n",
       " Row(name='Dan', year=2000, salary=5073.77),\n",
       " Row(name='Dan', year=2001, salary=4926.94),\n",
       " Row(name='Dan', year=2002, salary=5307.63),\n",
       " Row(name='Dan', year=2003, salary=6393.56),\n",
       " Row(name='Dan', year=2004, salary=6107.69),\n",
       " Row(name='Dan', year=2005, salary=6725.46),\n",
       " Row(name='Dan', year=2006, salary=7692.83),\n",
       " Row(name='Dan', year=2007, salary=8200.66),\n",
       " Row(name='Dan', year=2008, salary=8643.18),\n",
       " Row(name='Dan', year=2009, salary=9990.93),\n",
       " Row(name='Greg', year=2000, salary=4144.41),\n",
       " Row(name='Greg', year=2001, salary=5240.72),\n",
       " Row(name='Greg', year=2002, salary=6092.7),\n",
       " Row(name='Greg', year=2003, salary=5709.68),\n",
       " Row(name='Greg', year=2004, salary=6168.78),\n",
       " Row(name='Greg', year=2005, salary=6320.09),\n",
       " Row(name='Greg', year=2007, salary=6931.76),\n",
       " Row(name='Greg', year=2008, salary=7617.62),\n",
       " Row(name='Greg', year=2009, salary=8281.85),\n",
       " Row(name='Alice', year=3000, salary=2000.0)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salaryHistory.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Przyjrzyj sie nowemu zbiorowi danych salaryHistory.<br>\n",
    "a. Zobacz schemat. <br>\n",
    "b. Ile rekordow jest w calym zbiorze? <br>\n",
    "c. Jaka jest najmniejsza i najwieksza pensja?<br>\n",
    "d. Ile razy powtarza sie kazde z imion?<br>\n",
    "e. Stworz tabele sredniej, minimalnej i maksymalnej pensji w zależności od roku. Posortuj lata malejaco. Pensje podaj z dokladnoscia do pelnych wartosci. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaryHistory.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+------------------+\n",
      "|summary| name|              year|            salary|\n",
      "+-------+-----+------------------+------------------+\n",
      "|  count|   50|                50|                50|\n",
      "|   mean| null|           2024.38| 6839.503800000001|\n",
      "| stddev| null|140.81901686279477|2032.8277003368228|\n",
      "|    min|Alice|              2000|            2000.0|\n",
      "|    max| Greg|              3000|          10854.67|\n",
      "+-------+-----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaryHistory.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| name|count|\n",
      "+-----+-----+\n",
      "|Betty|   10|\n",
      "|Alice|   11|\n",
      "|Chris|   10|\n",
      "|  Dan|   10|\n",
      "| Greg|    9|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaryHistory.groupBy(\"name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---------+---------+\n",
      "|year|AverageSalary|MinSalary|MaxSalary|\n",
      "+----+-------------+---------+---------+\n",
      "|3000|       2000.0|   2000.0|   2000.0|\n",
      "|2009|       9773.0|   8282.0|  10855.0|\n",
      "|2008|       8972.0|   7618.0|   9862.0|\n",
      "|2007|       8617.0|   6932.0|  10848.0|\n",
      "|2006|       8083.0|   6569.0|   9649.0|\n",
      "|2005|       6982.0|   6320.0|   8165.0|\n",
      "|2004|       6269.0|   5631.0|   7394.0|\n",
      "|2003|       5816.0|   4969.0|   6520.0|\n",
      "|2002|       5433.0|   4761.0|   6190.0|\n",
      "|2001|       4928.0|   4133.0|   5761.0|\n",
      "|2000|       4738.0|   4093.0|   5667.0|\n",
      "+----+-------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaryHistory.groupBy('year').agg(f.round(f.avg('salary')).alias(\"AverageSalary\"),         \n",
    "f.round(f.min('salary')).alias(\"MinSalary\"),                 \n",
    "f.round(f.max('salary')).alias(\"MaxSalary\")).\\\n",
    "orderBy(f.desc('year')).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window functions\n",
    "over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do obliczania agregowanych wartosci w grupach definiowanych oknem (window).<br>\n",
    "Zwracaja wiele rekordow (tyle ile na wejsciu w grupie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partitionBy - definiuje podzial danych na okna<br>\n",
    "orderBy - definiuje sortowanie wewnatrz kazdego z okien<br>\n",
    "Frame (rangeBetween/rowsBetween) - definiuje offset<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "# from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partitionBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definicja 'okna'\n",
    "myWindowSpec = Window.partitionBy(allPeople['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcje agregujące okien (aka funkcje okien lub agregacje okienkowe ) <br>\n",
    "Funkcje okna działają w odniesieniu do grupy wierszy, nazywanej oknem, i obliczania wartości zwracanej dla każdego wiersza w oparciu o grupę wierszy. Funkcje okna są przydatne do przetwarzania zadań, takich jak Obliczanie średniej przenoszonej, obliczanie zbiorczej statystyki lub uzyskiwanie dostępu do wartości wierszy, w których podano względne położenie bieżącego wiersza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wywolanie funkcji na kazdym 'oknie'\n",
    "allPeople.withColumn('nameCount', f.count(allPeople['name']).over(myWindowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Do zbioru salaryHistory dodaj kolumne 'avgSalaryDiff', ktora bedzie zawierala roznice pomiedzy pensja z danego roku, a srednia pensja osoby na przestrzeni wszytskich lat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partitionBy + orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np. rank\n",
    "# - musimy zdefiniowac dodatkowo sortowanie wewnatrz kazdej z grup\n",
    "# - zwraca lp dla kolejnych rekordow posortowanych wedlug zadanych kolumn\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year'])\n",
    "ranked = (f.rank()).over(windowSpec)\n",
    "salaryHistory.withColumn('ranked', ranked).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryHistory_tmp = salaryHistory.filter(salaryHistory.name.isin('Alice', 'Greg'))\n",
    "salaryHistory_tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Dla zbioru salaryHistory, porownaj pensje ludzi pomiedzy najwczesniejszym a najpozniejszym rokiem ich pracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partitionBy + orderBy + rangeBetween/rowsBetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np. srednia ruchoma\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year']).rangeBetween(-1,0)\n",
    "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
    "salaryHistory.withColumn('movingAvg', movingAvg).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np. srednia ze wszystkich rekordow do aktualnego wlacznie\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year']).rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
    "salaryHistory.withColumn('movingAvg', movingAvg).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# podobny efekt uzyskamy ponizszym zapytaniam. Drobna roznica: rekordy w jednej grupie (imie, rok) nie zostana rozdzielone \n",
    "import sys\n",
    "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year']).rangeBetween(-sys.maxsize,0)\n",
    "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
    "salaryHistory.withColumn('movingAvg', movingAvg).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
